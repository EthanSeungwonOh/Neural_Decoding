{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rfHFdvnU21_i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612715294141,"user_tz":300,"elapsed":33515,"user":{"displayName":"Seungwon Oh","photoUrl":"https://lh4.googleusercontent.com/-Pykkk6KujHY/AAAAAAAAAAI/AAAAAAAAJnM/cVySNCcOVSA/s64/photo.jpg","userId":"13769519298017181104"}},"outputId":"c7436514-f96a-4156-9224-d72b62085af5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/github/Online_Learning_Neural_Decoding"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/github/Online_Learning_Neural_Decoding\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iPolWh0_J6Q2","executionInfo":{"status":"ok","timestamp":1612715299284,"user_tz":300,"elapsed":38650,"user":{"displayName":"Seungwon Oh","photoUrl":"https://lh4.googleusercontent.com/-Pykkk6KujHY/AAAAAAAAAAI/AAAAAAAAJnM/cVySNCcOVSA/s64/photo.jpg","userId":"13769519298017181104"}}},"source":["# from neural_decoding import data, models, utils\n","from neural_decoding.data import NeuralData\n","from neural_decoding.models import gru, lstm\n","from neural_decoding.utils import plot_cm"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"o93-57BTRbNO"},"source":["def train(data, model):\n","    \"\"\"grid search for a set of hyperparameters that leads to best f1-score performance on the data points\n","\n","    path:\n","    data_pts: list[tuple], list of (days, sessions) of datasets involved in this experiment\n","    train_params \n","    \"\"\"\n","    best_avg_f1_score = 0\n","    best_hists = 0\n","    best_hps = 0\n","    histories = [] \n","    metrics = []\n","    \n","    # use (1-X)% of label target dataset for fine-tuning\n","    X_val = X_data_pts[-1][n_train:]\n","    y_val = y_data_pts[-1][n_train:]\n","\n","    history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch, validation_data=(X_val, y_val), verbose=2)\n","    model2.fit(X_train, y_train, epochs=n_epochs, batch_size=batch, validation_data=(X_val, y_val), verbose=2)\n","\n","    y_train_bal = sum(y_train) / len(y_train)\n","    y_val_bal = sum(y_val) / len(y_val)\n","    print('train={}, {}, val={}, {}'.format(y_train.shape, y_train_bal, y_val.shape, y_val_bal)) \n","    print(\"is_transfer={}, time_step={}, lr_rate={}, n_neuron={}, batch={}\".format(is_transfer, time_step, lr_rate, n_neuron, batch))\n","    \n","    visual_train(history)\n","    y_pred = np.round(model.predict(X_val)) # predict on validation data\n","    accuracy, precision, recall, f1 = plot_cm(y_val, y_pred) \n","    print('validation metrics = {:.3f}, {:.3f}, {:.3f}, {:.3f}\\n'.format(accuracy, precision, recall, f1))\n","\n","    # fine-tune on current\n","    if is_transfer is True:\n","        for layer in model.layers[:-2]:\n","            layer.trainable = False\n","            print(layer.trainable)\n","        model.summary()                                      \n","        history = model.fit(X_tune, y_tune, epochs=30, batch_size=batch, validation_data=(X_val, y_val), verbose=2)\n","    \n","    visual_train(history)\n","    histories.append(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QL9PfhxGES0"},"source":["def main():\n","    h_params = {'lr': 1e-3, 'n_neurons':[100, 50], 'epochs':50, 'batch':32, 'split':0.5, 'w_decay':0.05, 'p':16}\n","\n","    # load dataset and preprocess it according to the model you want to use\n","    set1 = NeuralData('/content/drive/MyDrive/github/Online_Learning_Neural_Decoding/dataset', '1004', '1', '4') \n","    data1 = set1.load(2)\n","    data1['input'] = set1.handle_miss_val(data1['input'])\n","    data1 = set1.preprocess(data1, 10, 'recurrent')\n","\n","    # instantiate models for the baseline and our method \n","    lstm1 = gru(n_layers=0, input_dims = data1['input'], units=50, lr_rate=0.0001, r_drop_rate=0.25, drop_rate=0.1, opt='adam')\n","    lstm2 = gru(n_layers=0, input_dims = data1['input'], units=50, lr_rate=0.0001, r_drop_rate=0.25, drop_rate=0.1, opt='adam')\n","\n","    # pretrain both models on the first dataset\n","    lstm1.fit(data1['input'], data1['label'], epochs=h_params['epochs'], batch_size=h_params['batch'], verbose=2)\n","    lstm2.fit(data1['input'], data1['label'], epochs=h_params['epochs'], batch_size=h_params['batch'], verbose=2)\n","\n","    # baseline: predict on p samples of each future data block without model update\n","    for \n","        y_pred = model.predict(X_val) # predict on validation data\n","        accuracy, precision, recall, f1 = plot_cm(y_val, y_pred) \n","        print('validation metrics = {:.3f}, {:.3f}, {:.3f}, {:.3f}\\n'.format(accuracy, precision, recall, f1))\n","        metrics.append([accuracy, precision, recall, f1])      \n","\n","    # our method: continously predict p samples of each future data block and update the block\n","    for \n","        for \n","         y_pred = model.predict(X_val) # predict on validation data\n","        accuracy, precision, recall, f1 = plot_cm(y_val, y_pred) \n","        print('validation metrics = {:.3f}, {:.3f}, {:.3f}, {:.3f}\\n'.format(accuracy, precision, recall, f1))\n","        metrics.append([accuracy, precision, recall, f1])  \n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"px1aUNpWMIYe"},"source":["\n","    \n","    \n","\n","    # sceario 1: prediction on the \n","    for i in :\n","        lstm1.predict()\n","        \n","        for \n","            lstm2.predict()\n","            lstm2.fit()"],"execution_count":null,"outputs":[]}]}